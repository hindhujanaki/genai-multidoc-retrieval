{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9326e71f",
   "metadata": {
    "height": 931
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the model achieves reasonable passkey retrieval accuracy until around 33k to 34k context length. By extending the position interpolation to 48k, the model can handle longer documents, presenting moderate retrieval ability (60%-90% accuracy) in the range of 33k to 45k. However, there is a sharp accuracy degradation for the Llama2 7B model after the 4k context length. Additionally, the ablation study includes experiments on the number of fine-tuning steps and attention patterns, with other experimental results detailed in Section B of the appendix.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the model achieves reasonable passkey retrieval accuracy until around 33k to 34k context length. By extending the position interpolation to 48k, the model can handle longer documents, presenting moderate retrieval ability (60%-90% accuracy) in the range of 33k to 45k. However, there is a sharp accuracy degradation for the Llama2 7B model after the 4k context length. The ablation study includes experiments on the number of fine-tuning steps and attention patterns, with other experimental results detailed in Section B of the appendix.\n",
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It enables a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages, and its own generations using special tokens called reflection tokens. This approach allows the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art language models and retrieval-augmented models across various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to efficiently increase the context window of pre-trained models. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. It has shown strong empirical results on various tasks and models, demonstrating promising performance in fine-tuning on extremely large context lengths and retrieval tasks in long conversations. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by introducing trainable normalization and embedding layers, enabling the extension of LLMs to larger context lengths efficiently.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It enables a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages, and its own generations using special tokens called reflection tokens. This approach allows the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art language models and retrieval-augmented models across various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to efficiently increase the context window of pre-trained models. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. It has shown strong empirical results on various tasks and models, demonstrating promising performance in fine-tuning on extremely large context lengths and retrieval tasks in long conversations. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by introducing trainable normalization and embedding layers, enabling the extension of LLMs to larger context lengths efficiently.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It enables a language model to adaptively retrieve passages on-demand, generate and reflect on retrieved passages, and its own generations using special tokens called reflection tokens. This approach allows the language model to tailor its behavior to diverse task requirements, leading to significant performance improvements compared to state-of-the-art language models and retrieval-augmented models across various tasks such as open-domain QA, reasoning, and fact verification.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to efficiently increase the context window of pre-trained models. LongLoRA retains the original model architectures and is compatible with existing techniques like Flash-Attention2. It has shown strong empirical results on various tasks and models, demonstrating promising performance in fine-tuning on extremely large context lengths and retrieval tasks in long conversations. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by introducing trainable normalization and embedding layers, enabling the extension of LLMs to larger context lengths efficiently.\n",
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are used for training.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are used for training.\n",
      "assistant: The evaluation dataset used in MetaGPT includes three benchmarks: HumanEval, MBPP, and SoftwareDev. HumanEval consists of 164 handwritten programming tasks, MBPP comprises 427 Python tasks, and SoftwareDev is a collection of 70 representative software development tasks covering various scopes like mini-games, image processing algorithms, and data visualization.\n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as those from Petroni et al. (2021), Stelmakh et al. (2022), and Mihaylov et al. (2018). In total, 150k instruction-output pairs are used for training.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")\n",
    "\n",
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))\n",
    "\n",
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d981892",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
